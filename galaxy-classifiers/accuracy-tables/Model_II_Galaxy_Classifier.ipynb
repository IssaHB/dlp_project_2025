{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model II Galaxy Classifier (Spiral, Elliptical, Odd objects)\n",
        "\n",
        "**Most of this code is made by the authors of the paper of Ghaderi et al. (2025) (https://iopscience.iop.org/article/10.3847/1538-4365/ada8ab) and taken from the GitHub Repository: https://github.com/hmddev1/machine_learning_for_morphological_galaxy_classification**\n",
        "\n",
        "**This notebook classifies the images of galaxies to spiral and elliptical galaxies and odd objects using the Zernike Moments and the 1D-CNN (Model II).**\n",
        "\n",
        "**Purpose of this notebook: Run Model II for 10 iterations, and obtain results to reproduce table 4 of Ghaderi et al. (2025).**"
      ],
      "metadata": {
        "id": "YrbOYpXvNaOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmRUPNKnSlP0"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/Shared drives/DLP Project/Project/Models/Galaxy Models/Looping each Model'\n",
        "os.chdir(path)\n",
        "%run imports.py\n",
        "%matplotlib inline\n",
        "import plotting\n",
        "roc_curves = {}\n",
        "\n",
        "# Paths to images\n",
        "spath = r'/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_spiral'\n",
        "epath = r'/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_elliptical'\n",
        "opath = r'/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_odd'\n",
        "\n",
        "# Default image size and zernike order.\n",
        "image_size = 200\n",
        "zernike_order = 45\n",
        "\n",
        "# Loading the ZMs and concatenating to a consolidated dataset\n",
        "spiral_data = pd.read_csv('/content/drive/Shared drives/DLP Project/Project/spiral_zms.csv')\n",
        "elliptical_data = pd.read_csv('/content/drive/Shared drives/DLP Project/Project/elliptical_zms.csv')\n",
        "odd_data = pd.read_csv('/content/drive/Shared drives/DLP Project/Project/odd_zms.csv')\n",
        "\n",
        "spiral_data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "elliptical_data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "odd_data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "\n",
        "all_zm_data = np.concatenate([spiral_data, elliptical_data, odd_data])\n",
        "np.shape(all_zm_data)\n",
        "\n",
        "spiral_label = [0] * len(spiral_data)\n",
        "elliptical_label = [1] * len(elliptical_data)\n",
        "odd_label = [2] * len(odd_data)\n",
        "\n",
        "all_labels = np.concatenate([spiral_label, elliptical_label, odd_label])\n",
        "len(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VNeFR-RaXYSr"
      },
      "outputs": [],
      "source": [
        "num_iterations = 10\n",
        "metrics_list = []  # Store metrics for each iteration\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    print(f\"Iteration {iteration + 1}...\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
        "        all_zm_data, all_labels, np.arange(len(all_labels)),\n",
        "        test_size=0.25, shuffle=True, random_state=None)\n",
        "\n",
        "    y_train_encoded = to_categorical(y_train, num_classes=3)\n",
        "\n",
        "    class_weights = {0: len(all_zm_data) / (3 * len(spiral_data)),\n",
        "                     1: len(all_zm_data) / (3 * len(elliptical_data)),\n",
        "                     2: len(all_zm_data) / (3 * len(odd_data))}\n",
        "\n",
        "    # Building the model\n",
        "    x = Input(shape=(all_zm_data.shape[1], 1))\n",
        "\n",
        "    # Hidden layers\n",
        "    c0 = Conv1D(256, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "    b0 = BatchNormalization()(c0)\n",
        "    m0 = MaxPooling1D(pool_size=2)(b0)\n",
        "    d0 = Dropout(0.1)(m0)\n",
        "\n",
        "    c1 = Conv1D(128, kernel_size=3, strides=2, padding=\"same\")(d0)\n",
        "    b1 = BatchNormalization()(c1)\n",
        "    m1 = MaxPooling1D(pool_size=2)(b1)\n",
        "    d1 = Dropout(0.1)(m1)\n",
        "\n",
        "    c2 = Conv1D(64, kernel_size=3, strides=2, padding=\"same\")(d1)\n",
        "    b2 = BatchNormalization()(c2)\n",
        "    m2 = MaxPooling1D(pool_size=2)(b2)\n",
        "    d2 = Dropout(0.1)(m2)\n",
        "\n",
        "    f = Flatten()(d2)\n",
        "\n",
        "    # Output layer\n",
        "    de0 = Dense(64, activation='relu')(f)\n",
        "    de1 = Dense(32, activation='relu')(de0)\n",
        "    de2 = Dense(3, activation='softmax')(de1)\n",
        "\n",
        "    model = Model(inputs=x, outputs=de2, name=\"cnn_zm_45_galaxy_nonegalaxy\")\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "    b_size = 64\n",
        "    e_num = 30\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train_encoded,\n",
        "        batch_size=b_size,\n",
        "        epochs=e_num,\n",
        "        class_weight=class_weights,\n",
        "        verbose=1,\n",
        "        callbacks=es,\n",
        "        validation_split=0.1)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_labels)\n",
        "\n",
        "    # Performance metrics (per-class)\n",
        "    recall_per_class = recall_score(y_test, y_pred_labels, average=None)\n",
        "    precision_per_class = precision_score(y_test, y_pred_labels, average=None)\n",
        "    f1_per_class = f1_score(y_test, y_pred_labels, average=None)\n",
        "    accuracy = accuracy_score(y_test, y_pred_labels) # overall\n",
        "\n",
        "    # Compute per-class TSS\n",
        "    tss_per_class = {}\n",
        "    for i, class_name in enumerate(['Spiral', 'Elliptical', 'Odd']):\n",
        "        tp = cm[i, i]\n",
        "        fn = np.sum(cm[i, :]) - tp\n",
        "        fp = np.sum(cm[:, i]) - tp\n",
        "        tn = np.sum(cm) - (tp + fn + fp)\n",
        "        tss_per_class[class_name] = (tp / (tp + fn + 1e-6)) - (fp / (fp + tn + 1e-6))\n",
        "\n",
        "    # Store metrics in a list\n",
        "    metrics_list.append({\n",
        "        'Iteration': iteration + 1,\n",
        "        'Recall per Class': recall_per_class,\n",
        "        'Precision per Class': precision_per_class,\n",
        "        'F1 per Class': f1_per_class,\n",
        "        'Accuracy': accuracy,\n",
        "        'TSS per Class': tss_per_class\n",
        "    })\n",
        "\n",
        "\n",
        "for result in metrics_list:\n",
        "    print(result)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}