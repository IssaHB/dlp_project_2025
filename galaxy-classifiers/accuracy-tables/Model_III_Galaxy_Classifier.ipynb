{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11090645,
          "sourceType": "datasetVersion",
          "datasetId": 6913273
        },
        {
          "sourceId": 11091736,
          "sourceType": "datasetVersion",
          "datasetId": 6914118
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model III Galaxy Classifier (Spiral, Elliptical, Odd objects)\n",
        "\n",
        "**Most of this code is made by the authors of the paper of Ghaderi et al. (2025) (https://iopscience.iop.org/article/10.3847/1538-4365/ada8ab) and taken from the GitHub Repository: https://github.com/hmddev1/machine_learning_for_morphological_galaxy_classification**\n",
        "\n",
        "**This notebook classifies the images of galaxies to spiral and elliptical galaxies and odd objects using the Vision Transformer and the CNN (Model III).**\n",
        "\n",
        "**Purpose of this notebook: Run Model III for 10 iterations, and obtain results to reproduce table 4 of Ghaderi et al. (2025).**"
      ],
      "metadata": {
        "id": "-ckPjgaUQmWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = '/content/drive/Shared drives/DLP Project/Project/Models/Galaxy Models/Looping each Model'\n",
        "os.chdir(path)\n",
        "%run imports.py\n",
        "%matplotlib inline\n",
        "import plotting\n",
        "roc_curves = {}\n",
        "\n",
        "# Paths to images\n",
        "spath = r'/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_spiral'\n",
        "epath = r'/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_elliptical'\n",
        "opath = r'/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_odd'\n",
        "\n",
        "# Default image size and zernike order.\n",
        "image_size = 200\n",
        "zernike_order = 45\n",
        "\n",
        "# Loading the ZMs and concatenating to a consolidated dataset\n",
        "spiral_data = pd.read_csv('/content/drive/Shared drives/DLP Project/Project/spiral_zms.csv')\n",
        "elliptical_data = pd.read_csv('/content/drive/Shared drives/DLP Project/Project/elliptical_zms.csv')\n",
        "odd_data = pd.read_csv('/content/drive/Shared drives/DLP Project/Project/odd_zms.csv')\n",
        "\n",
        "spiral_data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "elliptical_data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "odd_data.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
        "\n",
        "all_zm_data = np.concatenate([spiral_data, elliptical_data, odd_data])\n",
        "np.shape(all_zm_data)\n",
        "\n",
        "spiral_label = [0] * len(spiral_data)\n",
        "elliptical_label = [1] * len(elliptical_data)\n",
        "odd_label = [2] * len(odd_data)\n",
        "\n",
        "all_labels = spiral_label + elliptical_label + odd_label\n",
        "len(all_labels)"
      ],
      "metadata": {
        "id": "D6z6octupbju",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T15:59:47.396462Z",
          "iopub.execute_input": "2025-03-27T15:59:47.396912Z",
          "iopub.status.idle": "2025-03-27T15:59:52.066101Z",
          "shell.execute_reply.started": "2025-03-27T15:59:47.396890Z",
          "shell.execute_reply": "2025-03-27T15:59:52.065156Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def load_galaxy_images(data_dir, target_size):\n",
        "        \"\"\"\n",
        "        Loads, resizes, and processes all JPG images from the specified directory.\n",
        "\n",
        "        Parameters:\n",
        "        data_dir (str): The directory containing the JPG images to be processed.\n",
        "        target_size (tuple): The target size for resizing the images, specified as (width, height).\n",
        "\n",
        "        Returns:\n",
        "        list: A list of PIL Image objects, each representing a resized and processed image.\n",
        "\n",
        "        The function performs the following steps:\n",
        "        1. Lists all JPG image files in the specified directory.\n",
        "        2. Reads each image using OpenCV.\n",
        "        3. Resizes each image to the specified target size.\n",
        "        4. Scales the pixel values and converts the image to a format compatible with PIL.\n",
        "        5. Converts each resized image to a PIL Image object.\n",
        "        6. Appends each PIL Image object to a list.\n",
        "        7. Returns the list of PIL Image objects.\n",
        "        \"\"\"\n",
        "\n",
        "        all_images = []\n",
        "\n",
        "        file_path = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir) if filename.endswith('.jpg')]\n",
        "\n",
        "        for img in file_path:\n",
        "            image = cv2.imread(img)\n",
        "            resized_images=cv2.resize(image, target_size)\n",
        "            resized_images = (resized_images * 255).astype(np.uint8)\n",
        "            pil_images = Image.fromarray(resized_images)\n",
        "            all_images.append(pil_images)\n",
        "\n",
        "        return all_images\n",
        "\n",
        "\n",
        "sp_dir = '/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_spiral'\n",
        "el_dir = '/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_elliptical'\n",
        "odd_dir = '/content/drive/Shared drives/DLP Project/Project/Data/galaxy/images/cropped_odd'\n",
        "\n",
        "image_size = 200\n",
        "\n",
        "sp_img = load_galaxy_images(sp_dir, target_size=(image_size,image_size))\n",
        "el_img = load_galaxy_images(el_dir, target_size=(image_size,image_size))\n",
        "odd_img = load_galaxy_images(odd_dir, target_size=(image_size,image_size))\n",
        "\n",
        "# Due to lack of computational power, we will use a subset of the data (6000 galaxies) while keeping the same ratios between categories.\n",
        "sp_img = sp_img[:3136]\n",
        "el_img = el_img[:2082]\n",
        "odd_img = odd_img[:782]\n",
        "\n",
        "all_data = sp_img + el_img + odd_img\n",
        "\n",
        "\n",
        "# Labels\n",
        "label_s = [0] * len(sp_img)\n",
        "label_e = [1] * len(el_img)\n",
        "label_o = [2] * len(odd_img)\n",
        "\n",
        "all_labels = label_s + label_e + label_o\n",
        "len(all_labels)\n",
        "\n",
        "\n",
        "# transforms for training data\n",
        "train_transform = transforms.Compose([transforms.CenterCrop(image_size),\n",
        "                                      transforms.RandomRotation(90),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomVerticalFlip(),\n",
        "                                      transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0), ratio=(0.99, 1.01)),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])\n",
        "\n",
        "# transforms for test data\n",
        "test_transform = transforms.Compose([transforms.CenterCrop(image_size),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "                                      ])"
      ],
      "metadata": {
        "id": "qN1zOnutpq5n",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T15:59:52.067490Z",
          "iopub.execute_input": "2025-03-27T15:59:52.067843Z",
          "iopub.status.idle": "2025-03-27T16:01:47.639626Z",
          "shell.execute_reply.started": "2025-03-27T15:59:52.067810Z",
          "shell.execute_reply": "2025-03-27T16:01:47.638668Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 10\n",
        "metrics_list = []  # Store metrics for each iteration\n",
        "all_roc_curves = []\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    print(f\"Iteration {iteration + 1}...\")\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(all_data, all_labels, np.arange(len(all_labels)), test_size=0.25, shuffle=True, random_state=None)\n",
        "    y_train_encoded = to_categorical(y_train, num_classes=3)\n",
        "\n",
        "    # # Transformer for training data\n",
        "    transformed_X_train = []\n",
        "    for i in range(len(X_train)):\n",
        "        transformed_train_images = train_transform(X_train[i])\n",
        "        new_image = np.transpose(transformed_train_images, (1, 2, 0))\n",
        "        transformed_X_train.append(new_image)\n",
        "\n",
        "    # # Transformer for testing data\n",
        "    transformed_X_test = []\n",
        "    for j in range(len(X_test)):\n",
        "        transformed_test_images = test_transform(X_test[j])\n",
        "        new_images = np.transpose(transformed_test_images, (1, 2, 0))\n",
        "        transformed_X_test.append(new_images)\n",
        "\n",
        "    # Class weights\n",
        "    class_weights = {0: len(all_data) / (3 * len(spiral_data)),\n",
        "                     1: len(all_data) / (3 * len(elliptical_data)),\n",
        "                     2: len(all_data) / (3 * len(odd_data))}\n",
        "\n",
        "    # Building the model\n",
        "\n",
        "    # input\n",
        "    x = Input(shape=(image_size, image_size, 3))\n",
        "\n",
        "    # hidden layers\n",
        "    c0 = Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
        "    b0 = BatchNormalization()(c0)\n",
        "    m0 = MaxPooling2D(pool_size=(2, 2))(b0)\n",
        "    d0 = Dropout(0.1)(m0)\n",
        "\n",
        "    c1 = Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(m0)\n",
        "    b1 = BatchNormalization()(c1)\n",
        "    m1 = MaxPooling2D(pool_size=(2, 2))(b1)\n",
        "    d1 = Dropout(0.1)(m1)\n",
        "\n",
        "    c2 = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(m1)\n",
        "    b2 = BatchNormalization()(c2)\n",
        "    m2 = MaxPooling2D(pool_size=(2, 2))(b2)\n",
        "    d2 = Dropout(0.1)(m2)\n",
        "\n",
        "    f = Flatten()(m2)\n",
        "\n",
        "    de0 = Dense(64, activation='relu')(f)\n",
        "    de1 = Dense(32, activation='relu')(de0)\n",
        "    de2 = Dense(3, activation='softmax')(de1)\n",
        "\n",
        "    model = Model(inputs=x, outputs=de2, name=\"cnn_transformer_galaxy_nonegalaxy\")\n",
        "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    # Training the model\n",
        "    b_size = 64\n",
        "    e_num = 30\n",
        "\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "    history = model.fit(\n",
        "        np.array(transformed_X_train), y_train_encoded,\n",
        "        batch_size=b_size,\n",
        "        epochs=e_num,\n",
        "        verbose=1,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=es,\n",
        "        validation_split=0.1)\n",
        "\n",
        "    y_pred = model.predict(np.array(transformed_X_test))\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred_labels)\n",
        "\n",
        "    # Performance metrics (per-class)\n",
        "    recall_per_class = recall_score(y_test, y_pred_labels, average=None)\n",
        "    precision_per_class = precision_score(y_test, y_pred_labels, average=None)\n",
        "    f1_per_class = f1_score(y_test, y_pred_labels, average=None)\n",
        "    accuracy = accuracy_score(y_test, y_pred_labels) # overall\n",
        "\n",
        "    # Compute per-class TSS\n",
        "    tss_per_class = {}\n",
        "    for i, class_name in enumerate(['Spiral', 'Elliptical', 'Odd']):\n",
        "        tp = cm[i, i]\n",
        "        fn = np.sum(cm[i, :]) - tp\n",
        "        fp = np.sum(cm[:, i]) - tp\n",
        "        tn = np.sum(cm) - (tp + fn + fp)\n",
        "        tss_per_class[class_name] = (tp / (tp + fn + 1e-6)) - (fp / (fp + tn + 1e-6))\n",
        "\n",
        "    # Store metrics in a list\n",
        "    metrics_list.append({\n",
        "        'Iteration': iteration + 1,\n",
        "        'Recall per Class': recall_per_class,\n",
        "        'Precision per Class': precision_per_class,\n",
        "        'F1 per Class': f1_per_class,\n",
        "        'Accuracy': accuracy,\n",
        "        'TSS per Class': tss_per_class\n",
        "    })\n",
        "\n",
        "for result in metrics_list:\n",
        "    print(result)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T16:01:47.641008Z",
          "iopub.execute_input": "2025-03-27T16:01:47.641268Z",
          "iopub.status.idle": "2025-03-27T16:43:26.823785Z",
          "shell.execute_reply.started": "2025-03-27T16:01:47.641245Z",
          "shell.execute_reply": "2025-03-27T16:43:26.822965Z"
        },
        "id": "BkxQp45CQWUU"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}